{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Basic Principles 2018 - Data Analysis Project Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Using linear SVC to classify music genres of an unbalanced dataset* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music genre classification is an interesting and relevant problem, where to possibility to automatically classify and sort music has many user applications. There is lot of well labeled data available and accessible. Before solving the actual classification problem the data needs to be analysed and pre-processed. Here this was done utilizing Matlab. The provided data set was cleaned of errors, and normalized according to feature group. At this point it was noted that the data set was unbalanced, which put constraints on the classification process. The classification process was done using the Sci-Kit Learn library in Python. As our final classifier we choose the linear Support Vector Classifier, as that yielded good results in the experiments. The accuracy of the model was tested by both accuracy of the predicted labels and the Log Loss metric. Although not excellent the chosen model managed to beat the benchmark and with some smart parameter choices achieve reasonably good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this project was to be able to correctly classify songs into genres based on a set of features gathered from each song. To do this, the songs feature sets have been divided into a training and a test set, and labels provided for the training set.\n",
    "\n",
    "The problem is an interesting machine learning task as the amount of data(songs) available worldwide is quite huge and the solution could have many applications, for example in providing users of different music services with a way to find new music based on a genre they like.\n",
    "\n",
    "It will also be a interesting learning process as it involve all parts of a Machine learning problem. The first task is to analyse the provided data, and pre-process it so that errors in the data set or large differences in value orders do not interfere with the classification process.\n",
    "\n",
    "The second part was to select an appropriate classification method. As there are quite a lot of different methods available there will be some need for experimentation and testing before finding good candidates to pursue further.\n",
    "\n",
    "The actual testing of the solution was tested via two [Kaggle](#refs) competitions. One for accuracy of the predicted labels and one for the Log Loss value of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the classification process the data set need to be understood and cleaned up. So that errors in the data set does not negatively affect the classification and before that the normalization process.\n",
    "\n",
    "The data visualization and clean up was done in Matlab. The script used to clean up the data set can be found in [Appendix A](#appA). The clean up and normalization choices are described below.\n",
    "\n",
    "As described in the DAP inctructions [document](#refs), the feature set consists of X larger parts with their separate bands and characteristics. The amount of different separate characteristics was not so large that manual inspection, via visualization, proved more resource efficient than implementing a checking algorithm.\n",
    "\n",
    "The features were visualized with [Matlab](#refs) on a characteristics basis, that is all bands of a characteristics were plotted as a line plot for each instance in the data set. For an example see the figure below. As can be seen from the figure the individual lines are not clear but if any data points were to be in another order of magnitude the plot would be skewed and it would be very noticeable. This method is of course only feasible with a small enough feature set that is well understood, but in this case it is good enough. Contour plots were also tried but to properly see the set of songs needed to be cut into small 100 count segments and that made it a not so practical approach.\n",
    "\n",
    "![Alt text](Viz.png \"Vizualization of Rythm means bands 1 to 24\")\n",
    "\n",
    "Another interesting aspect that could also be found while visualising the training set was the distribution of the labels, which is shown in the figure below. From the figure it is quite clear that the dataset is unbalanced and that class 1 is dominating. The provided [article](#refs) provided some ideas for how this could be tackled.\n",
    "\n",
    "![Alt text](dist_labels.png \"Distribution of labels\")\n",
    "\n",
    "Utilizing this method one instance of bad data was found. The first 4 bands on the YYYY characteristics were all 10 000 for all songs in the data set, whereas the remaining bands were in the range of 0 to 1, keeping the 10 000 in the data set would have largely skewed the dataset, when normalizing. There for it was concluded to set these to 0, as they were the same for all songs. Figures below shows the a contour map before and after the clean up for the first 50 songs.\n",
    "\n",
    "![Alt text](map_error.png \"Contour with error\")\n",
    "<center>*With errors uncorrected, all other values drowns out.*</center>\n",
    "\n",
    "\n",
    "![Alt text](map_norm.png \"Contour corrected and normalized\")\n",
    "<center>*With errors corrected, peaks visable.*</center>\n",
    "\n",
    "The normalization was also done in a controlled way. The feature set was normalized on a characteristics basis over all the bands belonging to that characteristics. For example features 1 to 24 were normalized with the maximum value found in this subgroup from all the songs. \n",
    "\n",
    "The test data set was also cleaned up and normalized in the same way. Afterwards the two sets were written in csv format for utilization in Python.\n",
    "\n",
    "[Appendix B](#appB) also explains the tested super sampling which did not in the end yield any better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "014a593ce82d342a60d749c7a2c46b7c",
     "grade": true,
     "grade_id": "cell-c3ef844c17cf4a1e",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before choosing a final method to implement in [scikit learn](#refs) in Python, we decided to try the data set in Matlab’s quick [classifier app](#refs) to test different approaches before moving over to the python environment.\n",
    "\n",
    "In Matlab the normalized training data was tested and validated using both the cross fold and hold out [method](#refs). In this case however there were very small differences between the hold out and cross fold method. The data was trained using many of the different classifier options and from this a few different methods were chosen, based on their accuracy scores. During this stage we also tested different feature reduction methods but this had an negative effect on the outcome. Probably because the data set was already quite small. Also undersampling was tested to see if the imbalance of the labels could be handled that way but the effects were worse probably due to the same reasons as with feature reduction. These methods were then further tested in Python. The methods that yielded high results were, Deep Trees and different degrees of Support Vector Machines([SVM](#refs)).\n",
    "\n",
    "Implementing the methods in Python using the scikit package was quite straight forward. And after some further testing the final method chosen was the [Support Vector Classifier](#refs) with balanced class weight option. At this stage the already mentioned super sampling technique was also ruled out, this might have been because the oversampling leads to a higher over fitting.\n",
    "\n",
    "Support vector Machines build on the principle of constructing hyperplanes in higher dimensional space in order to achieve separation between the classes. In cases where complete separation can’t be achieved the metode utilizes a so called soft margin approach. Where a [hinge loss](#refs) function. The mathematical formulation is:\n",
    "\n",
    "$$ \\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i $$\n",
    "\n",
    "$$ \\textrm{subject to } y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i, $$\n",
    "\n",
    "$$ \\zeta_i \\geq 0, i=1, ..., n $$\n",
    "\n",
    "The dual of this is: \n",
    "\n",
    "$$\\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha $$\n",
    "\n",
    "$$\\textrm{subject to } y^T \\alpha = 0$$\n",
    "\n",
    "$$ 0 \\leq \\alpha_i \\leq C, i=1, ..., n $$\n",
    "\n",
    "Here $e$ is a vector with all elements being 1, $C>0$ acts as an upper bound. Q is a semidefinte positive matrix which components map the training vector into the higher dimensional space via the kernel funtion, $K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)$, fur further information please see the [documentation](#refs).\n",
    "\n",
    "With a decision function of:\n",
    "\n",
    "$$\\operatorname{sgn}(\\sum_{i=1}^n y_i \\alpha_i K(x_i, x) + \\rho)$$\n",
    "\n",
    "It should be noted however that SVM methods works with binary classes, which in turn means that multi class problems, such as this is turned into a set of binary problems.\n",
    "\n",
    "To compute the problem mostly default parameters were used, the final choice used was `Calibrated Classifier CV` with `LinearSVC` estimate and weighted classes, with C set to .\n",
    "\n",
    "From the scikit SVC function its possible to get the needed output for the accuracy part directly, but for the log loss output the `CalibratedClassifierCV` function has the needed subfunction `.predict_proba` to be utilized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "import itertools\n",
    "\n",
    "\n",
    "#Read the samples\n",
    "X = np.asarray(pd.read_csv(\"Data.csv\"))\n",
    "#print(X)\n",
    "\n",
    "#Read the classes\n",
    "Y = np.ravel(np.asarray(pd.read_csv(\"train_labels.csv\")))\n",
    "#print(Y)\n",
    "\n",
    "test = np.asarray(pd.read_csv(\"Test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CalibratedClassifierCV(LinearSVC(random_state=0,class_weight='balanced',C=4,max_iter=2000),cv=5)\n",
    "sampleID = np.reshape(np.asarray(list(range(1,len(test)+1))),(6544,1))\n",
    "predictedValues = np.reshape(np.asarray(clf.fit(X,Y).predict(test)),(6544,1))\n",
    "logloss = np.reshape(np.asarray(clf.fit(X,Y).predict_proba(test)),(6544,10))\n",
    "e = clf.predict_proba(X)\n",
    "print('Probability score for train set: ' +str(clf.predict_proba(test)))\n",
    "print('Logloss Score: ' + str(log_loss(Y, e)))\n",
    "print('Accuracy Score: '+str(clf.score(X,Y)))\n",
    "\n",
    "\n",
    "result = np.append(sampleID,predictedValues,axis=1)\n",
    "#print(result)\n",
    "#np.savetxt(\"reslut.csv\",np.array(clf.fit(X,Y).predict(test)).astype(int),delimiter=\",\")\n",
    "with open(\"accuracy.csv\", \"wb\") as f:\n",
    "    f.write(b'Sample_id,Sample_label\\n')\n",
    "    np.savetxt(f, result.astype(int), fmt='%i', delimiter=\",\")\n",
    "\n",
    "\n",
    "result2 = np.append(sampleID,logloss.astype(float),axis=1)\n",
    "#print(result)\n",
    "#np.savetxt(\"reslut.csv\",np.array(clf.fit(X,Y).predict(test)).astype(int),delimiter=\",\")\n",
    "with open(\"logloss.csv\", \"wb\") as f:\n",
    "    f.write(b'Sample_id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9,Class_10\\n')\n",
    "    np.savetxt(f, result2, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results were achieved with the Calibrated Classifier CV with LinearSVC estimate and weighted classes. The achieved accuracy scores were 0.6917. This yielded a accuracy score of 0.59806 on kaggle.\n",
    "\n",
    "The log loss was similarly tested in Python using the built in functions which yielded a score of 0.9504503063156581 on the training set and later a score of 0.19165 in kaggle.\n",
    "\n",
    "The confusion matrix of the training set can be seen in the figure below. From the matrix it is clear that most errors comes from dealing with the 1st class, which is to be expected as that is the dominating one in the training set.\n",
    "\n",
    "![Alt text](confusion_matrix_One_VS_One.png \"Confusion matrix\")\n",
    "<center>*Confusion matrix of the training set.*</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "y_pred = clf.fit(X_train,Y_train).predict(X_test)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "cnf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix,normalize=True, classes= [1,2,3,4,5,6,7,8,9,10],\n",
    "                      title='Confusion matrix, with normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was quite a difference between the training data sets scores and the one from kaggle, but this is to be expected as the methods used has some inherited over fitting that is hard to get rid of. The big suprise though was the difference in the log loss function were the kaggel score was better than the pyhton internal for the training set. For this we have not been able to find a good explanation. \n",
    "\n",
    "The results could definitely be improved open but might require some different approaches already on the data preparation and normalization stage. The classifier method used might also not be the best for this kind of a problem. However the chosen method managed to beat the benchmark by quite a big marigne, 10 percentage points. Also as the dataset was quite small it is probably not a good idea to go more advance modern methods which usually requires quite large amounts of data.\n",
    "\n",
    "However one improvment that could have been done would have been to implment some form of parameter optimizer to finds the optimal parameters for the SVC, something like the `.GridSearchCV`([ref](#refs)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='refs'></a>\n",
    "**Kaggle** https://www.kaggle.com/\n",
    "\n",
    "\n",
    "\n",
    "**MLBP 2018 Data Analysis Project** https://mycourses.aalto.fi/pluginfile.php/743413/mod_resource/content/14/MLBP%202018%20project%20description.pdf\n",
    "\n",
    "**Mathworks Matlab** https://se.mathworks.com/help/matlab/index.html\n",
    "\n",
    "**8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset, Jason Brownlee, August 19, 2015, Machine Learning Process**\n",
    "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "**Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.** http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n",
    "\n",
    "**Matlab Classification learner app** https://se.mathworks.com/help/stats/classification-learner-app.html\n",
    "\n",
    "**Matlab Classification learner app, Select Data and Validation for Classification Problem**\n",
    "https://se.mathworks.com/help/stats/select-data-and-validation-for-classification-problem.html\n",
    "\n",
    "**Support vector machine**https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "**Scikit-learn:sklearn.svm.LinearSVC** http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "**Hinge Loss, Wikipedia** https://en.wikipedia.org/wiki/Hinge_loss\n",
    "\n",
    "**SVC mathematical explanation, SciKit-Learn** http://scikit-learn.org/stable/modules/svm.html#svc\n",
    "\n",
    "**Scikit-learn: sklearn.model_selection.GridSearchCV**\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A\n",
    "\n",
    "<a id='appA'></a>\n",
    "\n",
    "The matlab script utilized to clean up and normalize the data. \n",
    "\n",
    "```matlab\n",
    "%The training and test data sets need to be imported into matlab first\n",
    "%With variable names testdata and traindata\n",
    "Data = traindata;\n",
    "bands = 0:7;\n",
    "bands = bands*24;\n",
    "for i=1:7\n",
    "    max_B = max(Data{:,bands(i)+1:bands(i+1)},[],'all');\n",
    "    Data{:,bands(i)+1:bands(i+1)} = Data{:,bands(i)+1:bands(i+1)}/max_B;\n",
    "end\n",
    "\n",
    "bands = 0:4;\n",
    "bands = bands.*12;\n",
    "bands = bands+168;\n",
    "\n",
    "for i=1:4\n",
    "    max_B = max(Data{:,bands(i)+1:bands(i+1)},[],'all');\n",
    "    Data{:,bands(i)+1:bands(i+1)} = Data{:,bands(i)+1:bands(i+1)}/max_B;\n",
    "end\n",
    "\n",
    "bands = 0:4;\n",
    "bands = bands*12;\n",
    "bands = bands+216;\n",
    "Data{:,217:220} = 0;\n",
    "\n",
    "for i=1:4\n",
    "    max_B = max(Data{:,bands(i)+1:bands(i+1)},[],'all');\n",
    "    Data{:,bands(i)+1:bands(i+1)} = Data{:,bands(i)+1:bands(i+1)}/max_B;\n",
    "end\n",
    "\n",
    "writetable(Data,'Data.csv')\n",
    "%test noramlization\n",
    "Test = testdata;\n",
    "bands = 0:7;\n",
    "bands = bands*24;\n",
    "for i=1:7\n",
    "    max_B = max(Test{:,bands(i)+1:bands(i+1)},[],'all');\n",
    "    Test{:,bands(i)+1:bands(i+1)} = Test{:,bands(i)+1:bands(i+1)}/max_B;\n",
    "end\n",
    "\n",
    "bands = 0:4;\n",
    "bands = bands.*12;\n",
    "bands = bands+168;\n",
    "\n",
    "for i=1:4\n",
    "    max_B = max(Test{:,bands(i)+1:bands(i+1)},[],'all');\n",
    "    Test{:,bands(i)+1:bands(i+1)} = Test{:,bands(i)+1:bands(i+1)}/max_B;\n",
    "end\n",
    "\n",
    "bands = 0:4;\n",
    "bands = bands*12;\n",
    "bands = bands+216;\n",
    "Data{:,217:220} = 0;\n",
    "\n",
    "for i=1:4\n",
    "    max_B = max(Test{:,bands(i)+1:bands(i+1)},[],'all');\n",
    "    Test{:,bands(i)+1:bands(i+1)} = Test{:,bands(i)+1:bands(i+1)}/max_B;\n",
    "end\n",
    "\n",
    "writetable(Test,'Test.csv')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Appendix B\n",
    "\n",
    "<a id='appB'></a>\n",
    "\n",
    "One of the ides we tested before choosing the final method was to super sample the data set in an effort to combat the imbalance between the different classes. As we were afraid that this imbalance would lead bad accuracy, as was stated in the article posted on slack. This however did not in the ned lead to any noticeable increase in accuracy so the it was decided that it is better to go with the simplest solution that achieves good enough results. The matlab code used to generate the supersampled data set is anyway added for curiosa. \n",
    "\n",
    "\n",
    "```matlab\n",
    "%The training and test datasets as well as training labels need to be imported into matlab first\n",
    "%With variable names testdata and traindata, trainlabels\n",
    "index1 = find(trainlabels{:,:} == 1);\n",
    "index2 = find(trainlabels{:,:} == 2);\n",
    "index3 = find(trainlabels{:,:} == 3);\n",
    "index4 = find(trainlabels{:,:} == 4);\n",
    "index5 = find(trainlabels{:,:} == 5);\n",
    "index6 = find(trainlabels{:,:} == 6);\n",
    "index7 = find(trainlabels{:,:} == 7);\n",
    "index8 = find(trainlabels{:,:} == 8);\n",
    "index9 = find(trainlabels{:,:} == 9);\n",
    "index10 = find(trainlabels{:,:} == 10);\n",
    "\n",
    "N_target = size(index1,1);\n",
    "SData = Data;\n",
    "SLabels = trainlabels;\n",
    "\n",
    "N2 = size(index2,1);\n",
    "\n",
    "for i = 1:(N2)\n",
    "    j = randi(N2);\n",
    "    SData = [SData; SData(index2(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index2(j),:)];\n",
    "end\n",
    "\n",
    "N3 = size(index3,1);\n",
    "\n",
    "for i = 1:(N3)\n",
    "    j = randi(N3);\n",
    "    SData = [SData; SData(index3(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index3(j),:)];\n",
    "end\n",
    "\n",
    "N4 = size(index4,1);\n",
    "\n",
    "for i = 1:(N4)\n",
    "    j = randi(N4);\n",
    "    SData = [SData; SData(index4(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index4(j),:)];\n",
    "end\n",
    "\n",
    "N5 = size(index5,1);\n",
    "\n",
    "for i = 1:(N5)\n",
    "    j = randi(N5);\n",
    "    SData = [SData; SData(index5(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index5(j),:)];\n",
    "end\n",
    "\n",
    "N6 = size(index6,1);\n",
    "\n",
    "for i = 1:(N6)\n",
    "    j = randi(N6);\n",
    "    SData = [SData; SData(index6(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index6(j),:)];\n",
    "end\n",
    "\n",
    "N7 = size(index7,1);\n",
    "\n",
    "for i = 1:(N7)\n",
    "    j = randi(N7);\n",
    "    SData = [SData; SData(index7(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index7(j),:)];\n",
    "end\n",
    "\n",
    "N8 = size(index8,1);\n",
    "\n",
    "for i = 1:(N8)\n",
    "    j = randi(N8);\n",
    "    SData = [SData; SData(index8(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index8(j),:)];\n",
    "end\n",
    "%% \n",
    "\n",
    "N9 = size(index9,1);\n",
    "\n",
    "for i = 1:(N9)\n",
    "    j = randi(N9);\n",
    "    SData = [SData; SData(index9(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index9(j),:)];\n",
    "end\n",
    "\n",
    "N10 = size(index10,1);\n",
    "\n",
    "for i = 1:(N10)\n",
    "    j = randi(N10);\n",
    "    SData = [SData; SData(index10(j),:)];\n",
    "    SLabels = [SLabels; SLabels(index10(j),:)];\n",
    "end\n",
    "\n",
    "SLabels.Properties.VariableNames{1} = 'Labels';\n",
    "SData_m = [SData SLabels];\n",
    "\n",
    "writetable(SData, 'SData.csv')\n",
    "writetable(SLabels, 'SLabels.csv')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
